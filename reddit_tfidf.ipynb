{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.linalg import norm\n",
    "\n",
    "# local library\n",
    "from preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english') + [\"[applause]\", \"[music]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('speeches.json') as f:\n",
    "    speeches = json.load(f)\n",
    "bow = create_bow(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def my_tokenizer(document):\n",
    "        tk = WhitespaceTokenizer()\n",
    "        #stemmer = SnowballStemmer(\"english\")\n",
    "        # Break the sentence into tokens based on whitespace\n",
    "        for token in tk.tokenize(document):\n",
    "            # Apply preprocessing to the token\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "            token = token.replace('\"','')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in set(string.punctuation) for char in token):\n",
    "                continue\n",
    "                \n",
    "            if token in sw:\n",
    "                continue\n",
    "\n",
    "            # stem the token and yield\n",
    "            #stem = stemmer.stem(token)\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_comments = pd.read_csv('fr_comments.csv', usecols=['post_id','comment'])\n",
    "mr_comments = pd.read_csv('mr_comments2.csv', usecols=['post_id','comment'])\n",
    "ml_comments = pd.read_csv('ml_comments2.csv', usecols=['post_id','comment'])\n",
    "fl_comments = pd.read_csv('fl_comments2.csv', usecols=['post_id','comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments(df):\n",
    "    df = df.groupby('post_id').agg({'comment': lambda x: ' '.join(x)})\\\n",
    "            .reset_index().set_index('post_id')\n",
    "    # create document-term count matrix\n",
    "    vectorizer = CountVectorizer(#max_features=5000, # only top 5k words\n",
    "                                 min_df=2,          # words must appear in this many speeches to count\n",
    "                                 max_df=0.9,        # words can't appear in more % of speeches than this.  \n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 tokenizer=my_tokenizer,\n",
    "                                 ngram_range=(3,3)  # only bigrams\n",
    "                                )\n",
    "    counts = vectorizer.fit_transform(df.comment)\n",
    "    counts_df = pd.DataFrame(counts.toarray())\n",
    "    counts_df.index.name = 'post_id'\n",
    "    counts_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "    # convert counts into tfidf\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(counts)\n",
    "    # make it a nice df\n",
    "    tfidf_df = pd.DataFrame(tfidf.toarray())\n",
    "    tfidf_df.index.name = 'post_id'\n",
    "    tfidf_df.columns = vectorizer.get_feature_names()\n",
    "    \n",
    "    tfidf_group = tfidf_df.mean()\n",
    "\n",
    "    # Normalize doc vector lengths\n",
    "    tfidf_group = tfidf_group / norm(tfidf_group)\n",
    "\n",
    "    return tfidf_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tfidf = process_comments(fr_comments)\n",
    "mr_tfidf = process_comments(mr_comments)\n",
    "ml_tfidf = process_comments(ml_comments)\n",
    "fl_tfidf = process_comments(fl_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[removed] [removed] [removed]                                         0.287505\n",
       "please [contact moderators                                            0.148574\n",
       "[contact moderators subreddit](/message/compose/?to=/r/democrats)     0.148574\n",
       "moderators subreddit](/message/compose/?to=/r/democrats) questions    0.148574\n",
       "bot, action performed                                                 0.148574\n",
       "                                                                        ...   \n",
       "middle class well,                                                    0.006755\n",
       "no, i'm saying                                                        0.006735\n",
       "ever going happen.                                                    0.006289\n",
       "law says allowed                                                      0.006161\n",
       "feel like would                                                       0.005795\n",
       "Length: 1529, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_tfidf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5606,)\n",
      "(1779,)\n",
      "(1529,)\n",
      "(1225,)\n"
     ]
    }
   ],
   "source": [
    "print(fr_tfidf.shape)\n",
    "print(mr_tfidf.shape)\n",
    "print(ml_tfidf.shape)\n",
    "print(fl_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25522, 2)\n",
      "(19222, 2)\n",
      "(19236, 2)\n",
      "(15566, 2)\n"
     ]
    }
   ],
   "source": [
    "print(fr_comments.shape)\n",
    "print(mr_comments.shape)\n",
    "print(ml_comments.shape)\n",
    "print(fl_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at trigrams from speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document-term count matrix\n",
    "vectorizer = CountVectorizer(#max_features=5000, # only top 5k words\n",
    "                             min_df=3,          # words must appear in this many speeches to count\n",
    "                             max_df=0.9,        # words can't appear in more % of speeches than this.  \n",
    "                             stop_words=stopwords.words('english'),\n",
    "                             tokenizer=my_tokenizer,\n",
    "                             ngram_range=(3,3)  # only bigrams\n",
    "                            )\n",
    "counts = vectorizer.fit_transform(bow['speech'])\n",
    "counts_df = pd.DataFrame(counts.toarray())\n",
    "counts_df.index.name = 'speech'\n",
    "counts_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# convert counts into tfidf\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "# make it a nice df\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray())\n",
    "tfidf_df.index.name = 'speech'\n",
    "tfidf_df.columns = vectorizer.get_feature_names()\n",
    "tfidf_df['videoId'] = bow.index.values\n",
    "tfidf_df = tfidf_df.set_index('videoId')\n",
    "\n",
    "# group by speaker and aggregate\n",
    "tfidf_df['_speaker'] = tfidf_df.apply(lambda x: bow.loc[x.name].speaker, 1)\n",
    "tfidf_df = tfidf_df.reset_index().set_index(['videoId','_speaker'])\n",
    "# collapse to speaker-only\n",
    "tfidf_speaker = tfidf_df.reset_index().drop(columns=\"videoId\").groupby(['_speaker']).mean()\n",
    "# Normalize doc vector lengths\n",
    "tfidf_speaker = tfidf_speaker.apply(lambda x: x / norm(x), 1)\n",
    "# transpose for easier filtering\n",
    "tfidf_speaker = tfidf_speaker.T.reset_index()\n",
    "tfidf_speaker = tfidf_speaker.set_index('index')\n",
    "tfidf_speaker.index.name = 'term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fr_tfidf.to_frame(name=\"fr_tfidf\")\n",
    "df.index.name = \"term\"\n",
    "tfidf_speaker = tfidf_speaker.merge(df, left_index=True, right_index=True, how=\"left\").fillna(value=0)\n",
    "\n",
    "df = mr_tfidf.to_frame(name=\"mr_tfidf\")\n",
    "df.index.name = \"term\"\n",
    "tfidf_speaker = tfidf_speaker.merge(df, left_index=True, right_index=True, how=\"left\").fillna(value=0)\n",
    "\n",
    "df = ml_tfidf.to_frame(name=\"ml_tfidf\")\n",
    "df.index.name = \"term\"\n",
    "tfidf_speaker = tfidf_speaker.merge(df, left_index=True, right_index=True, how=\"left\").fillna(value=0)\n",
    "\n",
    "df = fl_tfidf.to_frame(name=\"fl_tfidf\")\n",
    "df.index.name = \"term\"\n",
    "tfidf_speaker = tfidf_speaker.merge(df, left_index=True, right_index=True, how=\"left\").fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biden</th>\n",
       "      <th>harris</th>\n",
       "      <th>pence</th>\n",
       "      <th>trump</th>\n",
       "      <th>fr_tfidf</th>\n",
       "      <th>mr_tfidf</th>\n",
       "      <th>ml_tfidf</th>\n",
       "      <th>fl_tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>president united states</th>\n",
       "      <td>0.114247</td>\n",
       "      <td>0.100319</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.053622</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>0.058226</td>\n",
       "      <td>0.023538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make america great</th>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.090591</td>\n",
       "      <td>0.028879</td>\n",
       "      <td>0.024063</td>\n",
       "      <td>0.056790</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social security medicare</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.021886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046712</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme court justice</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019644</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043449</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america great again.</th>\n",
       "      <td>0.016191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025334</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035821</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every single one</th>\n",
       "      <td>0.018445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.009939</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>0.041587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quid pro quo</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027212</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every single day</th>\n",
       "      <td>0.041058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047543</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.031832</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black lives matter</th>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.165029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054068</td>\n",
       "      <td>0.054169</td>\n",
       "      <td>0.171205</td>\n",
       "      <td>0.031238</td>\n",
       "      <td>0.026953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two years ago.</th>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green new deal</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031735</td>\n",
       "      <td>0.021114</td>\n",
       "      <td>0.031293</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every time see</th>\n",
       "      <td>0.023993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.028626</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay fair share</th>\n",
       "      <td>0.020152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028558</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every single time</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027976</td>\n",
       "      <td>0.007490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there's one thing</th>\n",
       "      <td>0.020710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what's going on.</th>\n",
       "      <td>0.017205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.023847</td>\n",
       "      <td>0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president donald trump</th>\n",
       "      <td>0.020348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581659</td>\n",
       "      <td>0.052312</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people would say</th>\n",
       "      <td>0.009446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even though know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022363</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall street journal</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             biden    harris     pence     trump  fr_tfidf  \\\n",
       "term                                                                         \n",
       "president united states   0.114247  0.100319  0.038685  0.053622  0.017452   \n",
       "make america great        0.009783  0.006666  0.023989  0.090591  0.028879   \n",
       "social security medicare  0.013350  0.021886  0.000000  0.008777  0.000000   \n",
       "supreme court justice     0.000000  0.038663  0.000000  0.019644  0.002016   \n",
       "america great again.      0.016191  0.000000  0.025334  0.002533  0.011739   \n",
       "every single one          0.018445  0.000000  0.003029  0.008420  0.009939   \n",
       "quid pro quo              0.000000  0.000000  0.000000  0.027212  0.004538   \n",
       "every single day          0.041058  0.000000  0.047543  0.018536  0.006958   \n",
       "black lives matter        0.023359  0.165029  0.000000  0.054068  0.054169   \n",
       "two years ago.            0.008610  0.000000  0.008435  0.002307  0.000000   \n",
       "green new deal            0.000000  0.031735  0.021114  0.031293  0.015789   \n",
       "every time see            0.023993  0.000000  0.011628  0.004932  0.008515   \n",
       "pay fair share            0.020152  0.000000  0.000000  0.010082  0.000000   \n",
       "every single time         0.000000  0.000000  0.009150  0.013616  0.000000   \n",
       "there's one thing         0.020710  0.000000  0.000000  0.004558  0.000000   \n",
       "what's going on.          0.017205  0.000000  0.000000  0.011289  0.005037   \n",
       "president donald trump    0.020348  0.000000  0.581659  0.052312  0.004362   \n",
       "people would say          0.009446  0.000000  0.000000  0.018803  0.000000   \n",
       "even though know          0.000000  0.000000  0.000000  0.006291  0.000000   \n",
       "wall street journal       0.000000  0.000000  0.000000  0.025335  0.000000   \n",
       "\n",
       "                          mr_tfidf  ml_tfidf  fl_tfidf  \n",
       "term                                                    \n",
       "president united states   0.010878  0.058226  0.023538  \n",
       "make america great        0.024063  0.056790  0.000000  \n",
       "social security medicare  0.000000  0.046712  0.000000  \n",
       "supreme court justice     0.000000  0.043449  0.000000  \n",
       "america great again.      0.000000  0.035821  0.000000  \n",
       "every single one          0.010084  0.033504  0.041587  \n",
       "quid pro quo              0.000000  0.032137  0.000000  \n",
       "every single day          0.026228  0.031832  0.000000  \n",
       "black lives matter        0.171205  0.031238  0.026953  \n",
       "two years ago.            0.000000  0.029061  0.000000  \n",
       "green new deal            0.040657  0.028705  0.000000  \n",
       "every time see            0.020921  0.028626  0.000000  \n",
       "pay fair share            0.000000  0.028558  0.000000  \n",
       "every single time         0.000000  0.027976  0.007490  \n",
       "there's one thing         0.000000  0.026840  0.000000  \n",
       "what's going on.          0.014553  0.023847  0.006956  \n",
       "president donald trump    0.000000  0.023017  0.000000  \n",
       "people would say          0.016903  0.022600  0.000000  \n",
       "even though know          0.000000  0.022363  0.000000  \n",
       "wall street journal       0.000000  0.021774  0.000000  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_speaker.sort_values('ml_tfidf', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = tfidf_speaker.loc[:,['biden','harris','pence','trump']]\n",
    "ideo = tfidf_speaker.loc[:,['fr_tfidf', 'mr_tfidf', 'ml_tfidf','fl_tfidf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biden</th>\n",
       "      <th>harris</th>\n",
       "      <th>pence</th>\n",
       "      <th>trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fr_tfidf</th>\n",
       "      <td>0.014402</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.021024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr_tfidf</th>\n",
       "      <td>0.031793</td>\n",
       "      <td>0.048131</td>\n",
       "      <td>0.010873</td>\n",
       "      <td>0.037080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ml_tfidf</th>\n",
       "      <td>0.028751</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.026056</td>\n",
       "      <td>0.030849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fl_tfidf</th>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.010886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             biden    harris     pence     trump\n",
       "fr_tfidf  0.014402  0.023000  0.008926  0.021024\n",
       "mr_tfidf  0.031793  0.048131  0.010873  0.037080\n",
       "ml_tfidf  0.028751  0.029785  0.026056  0.030849\n",
       "fl_tfidf  0.015091  0.012691  0.004009  0.010886"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideo.T.dot(speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
